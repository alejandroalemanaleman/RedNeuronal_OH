{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-21T10:59:50.009196Z",
     "start_time": "2024-12-21T10:59:48.845911Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "def random_initialization(layers_dims):\n",
    "    \"\"\"\n",
    "    Initialize weights and biases for any number of layers.\n",
    "\n",
    "    Args:\n",
    "    layers_dims -- List containing the dimensions of each layer, including input and output.\n",
    "\n",
    "    Returns:\n",
    "    parameters -- Dictionary containing weights and biases for all layers.\n",
    "    \"\"\"\n",
    "    parameters = {}\n",
    "    for l in range(1, len(layers_dims)):\n",
    "        parameters[f'W{l}'] = np.random.randn(layers_dims[l-1], layers_dims[l]) * 0.01\n",
    "        parameters[f'b{l}'] = np.zeros((1, layers_dims[l]))\n",
    "    return parameters\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # Stability improvement\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Perform forward propagation through the network.\n",
    "\n",
    "    Args:\n",
    "    X -- Input data.\n",
    "    parameters -- Dictionary containing weights and biases.\n",
    "\n",
    "    Returns:\n",
    "    activations -- Dictionary containing activations for each layer.\n",
    "    caches -- Dictionary containing linear outputs for each layer (useful for backpropagation).\n",
    "    \"\"\"\n",
    "    activations = {0: X}\n",
    "    caches = {}\n",
    "    L = len(parameters) // 2\n",
    "\n",
    "    for l in range(1, L + 1):\n",
    "        W = parameters[f'W{l}']\n",
    "        b = parameters[f'b{l}']\n",
    "        Z = np.dot(activations[l-1], W) + b\n",
    "        caches[l] = Z\n",
    "\n",
    "        if l == L:\n",
    "            activations[l] = softmax(Z)  # Output layer with softmax\n",
    "        else:\n",
    "            activations[l] = relu(Z)\n",
    "\n",
    "    return activations, caches\n",
    "\n",
    "def backward_propagation(X, Y, parameters, activations, caches):\n",
    "    \"\"\"\n",
    "    Perform backward propagation to calculate gradients.\n",
    "\n",
    "    Args:\n",
    "    X -- Input data.\n",
    "    Y -- True labels (one-hot encoded).\n",
    "    parameters -- Dictionary containing weights and biases.\n",
    "    activations -- Dictionary containing activations from forward propagation.\n",
    "    caches -- Dictionary containing linear outputs from forward propagation.\n",
    "\n",
    "    Returns:\n",
    "    gradients -- Dictionary containing gradients of weights and biases.\n",
    "    \"\"\"\n",
    "    gradients = {}\n",
    "    L = len(parameters) // 2\n",
    "    m = X.shape[0]\n",
    "\n",
    "    # Gradient for output layer\n",
    "    dZ = activations[L] - Y\n",
    "    for l in reversed(range(1, L + 1)):\n",
    "        W = parameters[f'W{l}']\n",
    "        dW = np.dot(activations[l-1].T, dZ) / m\n",
    "        db = np.sum(dZ, axis=0, keepdims=True) / m\n",
    "\n",
    "        gradients[f'dW{l}'] = dW\n",
    "        gradients[f'db{l}'] = db\n",
    "\n",
    "        if l > 1:\n",
    "            dZ = np.dot(dZ, W.T) * relu_derivative(caches[l-1])\n",
    "\n",
    "    return gradients\n",
    "\n",
    "def update_parameters(parameters, gradients, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent.\n",
    "\n",
    "    Args:\n",
    "    parameters -- Dictionary containing weights and biases.\n",
    "    gradients -- Dictionary containing gradients of weights and biases.\n",
    "    learning_rate -- Learning rate for gradient descent.\n",
    "\n",
    "    Returns:\n",
    "    parameters -- Updated parameters.\n",
    "    \"\"\"\n",
    "    for l in range(1, len(parameters) // 2 + 1):\n",
    "        parameters[f'W{l}'] -= learning_rate * gradients[f'dW{l}']\n",
    "        parameters[f'b{l}'] -= learning_rate * gradients[f'db{l}']\n",
    "    return parameters\n",
    "\n",
    "def compute_loss(Y_pred, Y_true):\n",
    "    \"\"\"\n",
    "    Compute the cross-entropy loss.\n",
    "\n",
    "    Args:\n",
    "    Y_pred -- Predicted probabilities from the model.\n",
    "    Y_true -- True labels (one-hot encoded).\n",
    "\n",
    "    Returns:\n",
    "    loss -- Cross-entropy loss.\n",
    "    \"\"\"\n",
    "    m = Y_true.shape[0]\n",
    "    loss = -np.sum(Y_true * np.log(Y_pred + 1e-8)) / m  # Add epsilon for numerical stability\n",
    "    return loss\n",
    "\n",
    "def train_model(X, Y, layers_dims, learning_rate=0.01, epochs=1000):\n",
    "    \"\"\"\n",
    "    Train the neural network with the specified architecture and parameters.\n",
    "\n",
    "    Args:\n",
    "    X -- Input data.\n",
    "    Y -- True labels (one-hot encoded).\n",
    "    layers_dims -- List specifying the number of neurons in each layer.\n",
    "    learning_rate -- Learning rate for gradient descent.\n",
    "    epochs -- Number of iterations for training.\n",
    "\n",
    "    Returns:\n",
    "    parameters -- Trained weights and biases.\n",
    "    losses -- List of loss values for each epoch.\n",
    "    \"\"\"\n",
    "    parameters = random_initialization(layers_dims)\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        activations, caches = forward_propagation(X, parameters)\n",
    "        loss = compute_loss(activations[len(layers_dims) - 1], Y)\n",
    "        losses.append(loss)\n",
    "\n",
    "        gradients = backward_propagation(X, Y, parameters, activations, caches)\n",
    "        parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}/{epochs} - Loss: {loss:.4f}\")\n",
    "\n",
    "    return parameters, losses\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T11:01:40.125490Z",
     "start_time": "2024-12-21T11:01:36.224445Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Cargar el conjunto de datos Iris\n",
    "data = load_iris()\n",
    "X = data.data  # Características (4)\n",
    "Y = data.target.reshape(-1, 1)  # Etiquetas (0, 1, 2)\n",
    "\n",
    "# Normalizar las características\n",
    "X = X / X.max(axis=0)\n",
    "\n",
    "# Codificar las etiquetas como one-hot\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "Y_onehot = encoder.fit_transform(Y)\n",
    "\n",
    "# Dividir en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y_onehot, test_size=0.2, random_state=42)\n",
    "\n",
    "# Definir la arquitectura de la red neuronal\n",
    "layers_dims = [X_train.shape[1], 10, 3]  # 4 (entrada), 10 (capa oculta), 3 (salida)\n",
    "\n",
    "# Entrenar el modelo\n",
    "parameters, losses = train_model(X_train, Y_train, layers_dims, learning_rate=0.01, epochs=1000)\n",
    "\n",
    "# Validar en el conjunto de prueba\n",
    "activations, _ = forward_propagation(X_test, parameters)\n",
    "Y_pred = np.argmax(activations[len(layers_dims) - 1], axis=1)\n",
    "Y_true = np.argmax(Y_test, axis=1)\n",
    "\n",
    "# Evaluar la precisión\n",
    "accuracy = np.mean(Y_pred == Y_true)\n",
    "print(f\"Precisión en el conjunto de prueba: {accuracy * 100:.2f}%\")\n"
   ],
   "id": "da4d268abe9aa183",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 15\u001B[0m\n\u001B[1;32m     12\u001B[0m X \u001B[38;5;241m=\u001B[39m X \u001B[38;5;241m/\u001B[39m X\u001B[38;5;241m.\u001B[39mmax(axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# Codificar las etiquetas como one-hot\u001B[39;00m\n\u001B[0;32m---> 15\u001B[0m encoder \u001B[38;5;241m=\u001B[39m \u001B[43mOneHotEncoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43msparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     16\u001B[0m Y_onehot \u001B[38;5;241m=\u001B[39m encoder\u001B[38;5;241m.\u001B[39mfit_transform(Y)\n\u001B[1;32m     18\u001B[0m \u001B[38;5;66;03m# Dividir en conjuntos de entrenamiento y prueba\u001B[39;00m\n",
      "\u001B[0;31mTypeError\u001B[0m: OneHotEncoder.__init__() got an unexpected keyword argument 'sparse'"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
